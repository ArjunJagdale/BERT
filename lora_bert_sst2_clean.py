# -*- coding: utf-8 -*-
"""LoRA_BERT_SST2_CLEAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KazgOFmv6kW_Bx6K7SUvmzbc0mxIH1bN

# Install & Import Libraries
"""

# âœ… Run this cell in Colab to install necessary packages
!pip install transformers datasets scikit-learn --quiet

import torch
import torch.nn as nn
from transformers import BertTokenizer, BertForSequenceClassification
from datasets import load_dataset
from sklearn.metrics import accuracy_score, classification_report
import numpy as np
import random
from torch.utils.data import DataLoader
from tqdm import tqdm

# âœ… Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

"""# Load SST-2 Dataset (via Hugging Face datasets)"""

# Load SST-2 (Stanford Sentiment Treebank)
dataset = load_dataset("glue", "sst2")

# Use pretrained tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenize function
def tokenize_fn(example):
    return tokenizer(
        example["sentence"],
        padding="max_length",
        truncation=True,
        max_length=128,
    )

# Apply tokenization
tokenized = dataset.map(tokenize_fn, batched=True)
tokenized.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

"""# Train/Val Split & DataLoader"""

train_data = tokenized["train"]
val_data = tokenized["validation"]

train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
val_loader = DataLoader(val_data, batch_size=64)

"""# Define LoRA Module & Inject into BERT Attention"""

class LoRALinear(nn.Module):
    def __init__(self, original_linear: nn.Linear, r: int = 4, alpha: int = 16):
        super().__init__()
        self.in_features = original_linear.in_features
        self.out_features = original_linear.out_features
        self.r = r
        self.alpha = alpha

        # Original frozen weight
        self.weight = original_linear.weight
        self.bias = original_linear.bias

        # LoRA adapters (A: down-projection, B: up-projection)
        self.A = nn.Parameter(torch.randn(r, self.in_features) * 0.01)
        self.B = nn.Parameter(torch.randn(self.out_features, r) * 0.01)

        # Scaling factor
        self.scaling = self.alpha / self.r

        # Freeze the original weight
        self.weight.requires_grad = False
        if self.bias is not None:
            self.bias.requires_grad = False

    def forward(self, x):
        # LoRA: W(x) + alpha/r * BA(x)
        lora_update = (x @ self.A.T) @ self.B.T
        return nn.functional.linear(x, self.weight) + self.scaling * lora_update

"""# Inject LoRA into BERT Attention Layers"""

def inject_lora_into_bert(model, r=4, alpha=16):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) and ("attention.self.query" in name or "attention.self.value" in name):
            parent = get_parent_module(model, name)
            layer_name = name.split(".")[-1]
            setattr(parent, layer_name, LoRALinear(module, r=r, alpha=alpha))

def get_parent_module(model, module_name):
    components = module_name.split(".")
    for comp in components[:-1]:
        model = getattr(model, comp)
    return model

"""## Load Pretrained BERT & Inject LoRA"""

lora_config = {"r": 4, "alpha": 16}

# Load BERT-base for binary classification
model_lora = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
inject_lora_into_bert(model_lora, **lora_config)
model_lora.to(device)

"""## Freeze All But LoRA Parameters"""

# Freeze all parameters
for param in model_lora.parameters():
    param.requires_grad = False

# Enable training only for LoRA adapters
for name, param in model_lora.named_parameters():
    if "A" in name or "B" in name:
        param.requires_grad = True

"""# Train LoRA-injected BERT on SST-2"""

def train(model, dataloader, optimizer, criterion):
    model.train()
    total_loss = 0
    for batch in tqdm(dataloader, desc="Training", leave=False):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    return total_loss / len(dataloader)

def evaluate(model, dataloader):
    model.eval()
    preds, labels = [], []
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels.extend(batch["label"].cpu().numpy())

            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            pred = torch.argmax(logits, dim=-1).cpu().numpy()
            preds.extend(pred)

    acc = accuracy_score(labels, preds)
    report = classification_report(labels, preds, output_dict=True)
    return acc, report

from torch.optim import AdamW
from torch.nn import CrossEntropyLoss

optimizer = AdamW(filter(lambda p: p.requires_grad, model_lora.parameters()), lr=1e-4)
criterion = CrossEntropyLoss()

epochs = 5
for epoch in range(epochs):
    print(f"\nEpoch {epoch + 1}/{epochs}")
    train_loss = train(model_lora, train_loader, optimizer, criterion)
    val_acc, _ = evaluate(model_lora, val_loader)

    print(f"Train Loss: {train_loss:.4f} | Validation Accuracy: {val_acc:.4f}")

"""# Comparison â€” LoRA vs Full Fine-Tuning"""

def count_trainable(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

lora_params = count_trainable(model_lora)

print(f"LoRA Trainable Params: {lora_params:,}")

lora_acc, lora_report = evaluate(model_lora, val_loader)

print(f"\nðŸ“Š Final Accuracy:")
print(f"LoRA       : {lora_acc:.4f}")

from sklearn.metrics import classification_report

print("\nðŸ§¾ LoRA Classification Report:")
print(lora_report)

"""# Inference on Real Sentences"""

def predict_sentiment(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128).to(device)

    with torch.no_grad():
        logits = model(**inputs).logits
        probs = torch.softmax(logits, dim=-1)
        pred = torch.argmax(probs, dim=-1).item()
        confidence = probs[0, pred].item()

    label = "Positive" if pred == 1 else "Negative"
    return label, confidence

sentences = [
    "The movie was fantastic and thrilling!",
    "I wouldn't recommend it to anyone.",
    "my views are neutral",
    "This is one of the best performances I've seen.",
    "The film lacked a solid storyline.",
]

print("ðŸ§  Inference Results\n")
for text in sentences:
    lora_label, lora_conf = predict_sentiment(text, model_lora, tokenizer)

    print(f"ðŸ”¹ Sentence: {text}")
    print(f"   LoRA âž¤ {lora_label} ({lora_conf:.2f})")

def merge_lora_weights(model):
    """
    Merges LoRA adapters (A,B) into the frozen base weights for deployment.
    """
    for name, module in model.named_modules():
        if isinstance(module, LoRALinear):
            # Compute merged weight: W + (alpha/r)*BA
            merged_weight = module.weight + (module.scaling * module.B @ module.A)
            # Replace module with a standard nn.Linear containing merged weights
            merged_linear = nn.Linear(module.in_features, module.out_features)
            merged_linear.weight = nn.Parameter(merged_weight)
            if module.bias is not None:
                merged_linear.bias = nn.Parameter(module.bias)
            parent = get_parent_module(model, name)
            layer_name = name.split(".")[-1]
            setattr(parent, layer_name, merged_linear)
    return model

# Merge adapters
model_merged = merge_lora_weights(model_lora)

# Save HuggingFace-style directory
save_dir = "./merged_lora_model"
model_merged.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

print(f"âœ… Merged model exported to {save_dir}")

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Path to your exported model directory
model_path = "./merged_lora_model"

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)
model.eval()

def predict(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    with torch.no_grad():
        logits = model(**inputs).logits
    pred = torch.argmax(torch.softmax(logits, dim=-1), dim=-1).item()
    return "Positive" if pred == 1 else "Negative"

# Test locally in Colab
examples = [
    "The movie was absolutely brilliant!",
    "I wasted two hours watching this.",
    "It was okay, nothing special.",
]

for t in examples:
    print(f"{t} â†’ {predict(t)}")

